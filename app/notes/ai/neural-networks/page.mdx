
# Neural Networks   

## Resizable Table of Contents


## 1. A neural network is a composition of functions

A neural network is a stack of functions applied one after another:

$$
f(\mathbf{x}) = f_L\circ f_{L-1}\circ \dots \circ f_1(\mathbf{x})
$$

Each function transforms a vector into another vector.

---

## 2. Each layer is a linear transformation plus a nonlinearity

* A layer is defined by:
    * Two parameters:
        * **$W$** weight matrix
        * **$\mathbf{b}$** bias vector
    
    * An activation function ( $\sigma$ )
    
        **$f(\mathbf{a}) = \sigma(W\mathbf{a} + \mathbf{b})$**
    


* Deep Dive: a is the input vector to a layer

    For layer $l$:
    
    * $\mathbf{a}^{(l-1)}$ = the vector produced by the previous layer
    * $\mathbf{a}^{(l)}$ = the vector after applying
      $$
      \mathbf{a}^{(l)} = \sigma\left(W^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}\right)
      $$
    
    So **each $\mathbf{a}$** is simply the **current activation vector**, i.e., the current layer’s output.



---

## 3. The whole network is just repeated application of this rule

For layer ( l ):

$$
\mathbf{a}^{(l)} = \sigma\left(W^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}\right)
$$

Starting from the input vector
$$
\mathbf{a}^{(0)} = \mathbf{x}
$$




---

## 4. Training is minimizing a loss over these parameters

Given training examples ($(\mathbf{x}, \mathbf{y})$), training chooses the matrices and biases that minimize a loss:

$$
\mathcal{L} = \text{Loss}(f(\mathbf{x}), \mathbf{y})
$$

Optimization uses gradients:

$$
W \leftarrow W - \eta \frac{\partial \mathcal{L}}{\partial W}, \qquad
b \leftarrow b - \eta \frac{\partial \mathcal{L}}{\partial b}.
$$

---

## 5. Activation functions introduce nonlinearity

Examples:

* ReLU:
  $$
  \sigma(z) = \max(0, z)
  $$

* Sigmoid:
  $$
  \sigma(z) = \frac{1}{1+e^{-z}}
  $$

They enable the network to represent complex functions.

---

## 6. The network computes a differentiable function

Because the entire structure is:

* matrix multiplication → affine transformation
* elementwise nonlinearity
* repeated composition

the full network ( $f(\mathbf{x})$ ) is **differentiable**, which makes gradient-based learning possible.

---
