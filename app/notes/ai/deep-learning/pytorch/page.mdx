
# PyTorch Cheat Sheet

## Resizable Table of Contents

*PyTorch* is a machine learning library, like TensorFlow.

At its core, PyTorch provides an interface for creating and manipulating tensors,
which are data structures that you can think of as multi-dimensional arrays. 
Tensors are represented as *n-dimensional* arrays of base datatypes such as a 
string or integer -- they provide a way to generalize vectors and matrices 
to higher dimensions. 

PyTorch provides the ability to perform computation on these tensors, 
define neural networks, and train them efficiently.

The shape of a PyTorch tensor defines its number of dimensions and 
the size of each dimension. 
The `ndim` or `dim` of a PyTorch tensor provides
 the number of dimensions (*n-dimensions*) -- this is equivalent to the tensor's rank (as is used in TensorFlow), and you can also think of this as the tensor's order or degree.

Let’s start by creating some tensors and inspecting their properties:



## 1. Core Ideas (Concepts)

### 1.1 Tensors
	PyTorch’s core data structure; like NumPy arrays, but can live on CPU or GPU.

### 1.2 Autograd
	Tracks tensor operations and can compute gradients with tensor.backward().

### 1.3 nn.Module (Models)
	Base class for all models: create a class that subclasses nn.Module and implement forward().

### 1.4 Loss Functions
	Measure how wrong the model is, e.g. nn.CrossEntropyLoss(), nn.MSELoss().

### 1.5 Optimizers
	Use gradients to update parameters, e.g. optim.SGD(...), optim.Adam(...).

---

## 2. Working with Tensors

### 2.1 Tensor Creation
	•	torch.tensor(data)
	•	torch.zeros(shape)
	•	torch.ones(shape)
	•	torch.rand(shape)

### 2.2 Tensor Manipulation
	•	x.view(shape) – reshape (needs contiguous).
	•	x.reshape(shape) – safer reshape.
	•	x.t() – transpose 2D.
	•	x.permute(*dims) – reorder axes.
	•	x.squeeze() / x.unsqueeze(dim) – remove/add size-1 dimensions.


---

## 3. Devices (CPU vs GPU)

### 3.1 Checking and Choosing Device
	•	torch.cuda.is_available()
	•	device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

### 3.2 Moving Data and Models
	•	x = x.to(device)
	•	model = model.to(device)

---

## 4. Building Neural Networks

### 4.1 Common Layers
	•	Linear: nn.Linear(in_features, out_features)
	•	Convolution: nn.Conv1d, nn.Conv2d, nn.Conv3d

### 4.2 Activation Functions
	•	nn.ReLU(), nn.Tanh(), nn.Sigmoid() (now mostly for special cases).

### 4.3 Losses
	•	Classification: nn.CrossEntropyLoss()
	•	Regression: nn.MSELoss()

### 4.4 Optimizers
	•	optim.SGD(model.parameters(), lr=0.01)
	•	optim.Adam(model.parameters(), lr=0.001)


## 5. Training Loop (End-to-End)

### 5.1 Data
	•	Wrap your dataset with torch.utils.data.Dataset and DataLoader.

### 5.2 Typical Training Step
	1.	Forward: y_pred = model(x)
	2.	Loss: loss = criterion(y_pred, y)
	3.	Zero grads: optimizer.zero_grad()
	4.	Backward: loss.backward()
	5.	Step: optimizer.step()

