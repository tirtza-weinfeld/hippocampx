# Machine Learning

## Resizable Table of Contents

Machine learning is the discipline of building systems that **learn patterns from data** rather than relying on explicitly programmed rules.  
A machine-learning system receives **examples** and tries to infer a function that generalizes beyond those examples.

There are three major paradigms:

- **[Supervised Learning](#1-supervised-learning)** — learning from labeled examples  
- **[Unsupervised Learning](#2-unsupervised-learning)** — finding structure without labels  
- **[Reinforcement Learning](#3-reinforcement-learning)** — learning by interacting with an environment and receiving rewards  

---

## 1. Supervised Learning

Supervised learning provides the model with **input–output pairs**.  
The goal is to learn a mapping:
$$
h(x) \; \rightarrow \; y
$$

where:
- $x$ = input features  
- $y$ = correct label  
- $h$ = hypothesis/model  

Supervised learning generally splits into **classification** and **regression**.

---

### 1.1 Supervised Learning Task Types
- Deep Dive: Classification: 
    - supervised learning task of learning a function mapping an input point to adiscrete category 
- Deep Dive: Regression: 
  - supervised learning task of learning a function mapping an input point to a continuous value


#### 1.1.1 Classification

Task: assign each input to **one of several discrete categories**.

Examples:
- Email → spam / not spam  
- Image → “cat”, “dog”, or “bird”  
- Medical record → “disease present” / “disease absent”  

A classifier tries to find boundaries in the feature space that separate classes.

---

#### 1.1.2 Regression

Task: output a **continuous numeric value**.

Examples:
- Predicting house price  
- Forecasting temperature  
- Estimating probability that it rains tomorrow  

A regression model fits a function like a line or curve that best approximates the data.

---

### 1.2. Core Algorithms in Supervised Learning

#### 1.2.1 k-Nearest Neighbors (k-NN)

- Store the training data.  
- For a new point, find the **k closest training points**.  
- Predict the majority class (classification) or mean value (regression).  

A lazy learner: no training phase — computation happens at prediction time.

---

#### 1.2.2 Perceptron (Linear Classifier)

> [!note:collapse] 
    A **linear classifier** is a model that makes its decision by checking **which side of a hyperplane** an input falls on.  

    Formally, it computes:
    
    $$
    s(x) = W \cdot x + b
    $$
    
    and classifies based on the **sign** of this linear score:
    
    * if $s(x) \ge 0$ → Class 1
    * if $s(x) < 0$ → Class 0
    
    The boundary between the two classes is the hyperplane:
    
    $$
    W \cdot x + b = 0
    $$
    
    This hyperplane is **linear** (a line in 2D, a plane in 3D, etc.), which is why the classifier is called *linear*.
    
    Examples of linear classifiers:
    
    * Perceptron
    * Logistic regression
    * Linear SVM
    * Linear discriminant analysis (LDA)
    
    If you want, I can rewrite this in one line to fit your section style.
 


The perceptron is a **linear classifier** that learns a **hyperplane** separating two classes.
It works in **any number of dimensions**.

For an input vector
$$
x = (1, x_1, x_2, \dots, x_d)
$$
and weight vector
$$
W = (w_0, w_1, w_2, \dots, w_d),
$$
the perceptron computes the **score**:

$$
W \cdot x = w_0 + w_1 x_1 + \dots + w_d x_d
$$

This score tells us which side of the hyperplane the point lies on.
The hyperplane itself is the set of points where:

$$
W \cdot x = 0
$$

This is the **geometric idea** behind all perceptron variants:

* the weights define a separating hyperplane
* classification is determined by which side of that hyperplane the input falls on
* training adjusts the weights to improve separation

**This is perceptron in general** — it applies to all linear classifiers, including both hard and soft perceptrons.

---

#####  1.2.2.1 Hard-Threshold Perceptron

The hard perceptron uses a **binary step function** to make its decision:

$$
h_W(x) =
\begin{cases}
1 & \text{if } W \cdot x \ge 0 \\
0 & \text{otherwise}
\end{cases}
$$

Training updates weights only when the model misclassifies a point:

$$
W_i \leftarrow W_i + \alpha ,(y - h_W(x)), x_i
$$

* outputs are exactly 0 or 1
* cannot express uncertainty
* classic Rosenblatt perceptron

---

#####  1.2.2.2 Soft Perceptron (Logistic / Sigmoid Version)

Replace the step with the **sigmoid** activation:

$$
\sigma(z) = \frac{1}{1 + e^{-z}},
\qquad h_W(x) = \sigma(W \cdot x)
$$

Now the output is a **probability** in $(0,1)$.

Weights are updated using gradient descent:

$$
W_i \leftarrow W_i + \alpha ,(y - \sigma(W \cdot x)), x_i
$$

This version

* is smooth and differentiable
* works even when data are not linearly separable
* expresses uncertainty




---

#### 1.2.3 Support Vector Machines (SVMs)

The goal of an SVM is to find a separating hyperplane with the **largest possible margin**.

Key idea:
- The optimal boundary is the one farthest from the closest points of each class.
- Leads to better generalization and robustness.

Often solved using convex optimization.

---

### 1.3 Loss Functions

A **loss function** measures how wrong a prediction is.

Examples:
- **0–1 Loss** (classification): 1 if incorrect, 0 if correct  
- **L1 Loss** (regression): $|\hat{y} - y|$  
- **L2 Loss** (regression): $(\hat{y} - y)^2$

A learning algorithm attempts to **minimize** total loss over training data.

---

### 1.4 Regularization & Model Complexity

Goal: prevent overly complex models that memorize the training data.

A **regularized cost** might be:

$$
\text{Cost}(h) = \text{Loss}(h) + \lambda \cdot \text{Complexity}(h)
$$

Common complexity measures:
- Size of weights  
- Depth of model  
- Number of parameters  

Regularization reduces overfitting.

---

### 1.5 Training, Validation, Testing

To evaluate how well a model generalizes:

1. **Training Set** — used to learn parameters  
2. **Validation Set** — used to tune hyperparameters  
3. **Test Set** — used only at the end to measure true performance  

**k-Fold Cross-Validation** repeats training/testing across multiple splits for more reliable evaluation.

---

## 2. Unsupervised Learning

Unsupervised learning uses **unlabeled data**.  
The model tries to uncover **patterns or structure** within the dataset.

Common tasks:
- Clustering  
- Dimensionality reduction  
- Density estimation  

---

#### 2.1 k-Means Clustering

    k-Means partitions data into $k$ clusters based on distance.

Algorithm:
1. Choose $k$ initial cluster centers (centroids).  
2. Assign each point to the nearest centroid.  
3. Recompute each centroid as the mean of its assigned points.  
4. Repeat steps 2–3 until assignments stop changing.

Outputs:
- A set of clusters  
- Centroids representing group centers  

Used heavily in segmentation, compression, and exploratory analysis.

---

## 3. Reinforcement Learning

Reinforcement learning features an **agent** interacting with an **environment** over time.

At each step:
- The agent observes state $s$.  
- Takes an action $a$.  
- Receives a reward $r$.  
- Moves to a new state $s'$.  

The goal is to **maximize cumulative long-term reward**.

---

#### 3.1 Markov Decision Processes (MDPs)

An MDP is defined by:
- **States** $S$  
- **Actions** $A(s)$ available in each state  
- **Transition probabilities** $P(s' | s, a)$  
- **Reward function** $R(s, a, s')$  
- **Discount factor** $\gamma$ determining how much future rewards matter  

Reinforcement-learning algorithms operate on top of this structure.

---

#### 3.2 Q-Learning

Q-Learning learns a function:

$$
Q(s, a) = \text{expected long-term reward when taking action } a \text{ in state } s
$$

Core update rule:

$$
Q(s,a) \leftarrow Q(s,a)
+ \alpha \Big( r + \gamma \max_{a'} Q(s',a') - Q(s,a) \Big)
$$

Where:
- $\alpha$ = learning rate  
- $\gamma$ = discount factor  
- $r$ = reward  
- $s'$ = next state  

This update gradually improves the agent’s value estimates.

---

#### 3.3 Exploration vs. Exploitation

A fundamental tradeoff:

- **Exploit**: choose the currently best-estimated action  
- **Explore**: try new actions to gather more information  

A common strategy is **ε-greedy**:
- With probability $\epsilon$ → random action  
- With probability $1 - \epsilon$ → best known action  

Balancing these leads to optimal learning.

---

##  Summary Table

| Category               | Uses Labels? | Purpose                             | Representative Methods |
|------------------------|--------------|--------------------------------------|-------------------------|
| **Supervised**         | Yes          | Predict labels/values                | k-NN, Perceptron, SVM, Regression |
| **Unsupervised**       | No           | Discover structure                    | k-Means, clustering     |
| **Reinforcement**      | Reward only  | Maximize long-term reward             | Q-Learning, MDP         |

---

## Key Concepts Cheatsheet

- **Hypothesis**: a model that maps inputs to outputs  
- **Loss**: error on a single example  
- **Cost**: total error (plus regularization)  
- **Overfitting**: memorizing training data instead of generalizing  
- **Underfitting**: model too simple to learn patterns  
- **Bias term**: allows shifting the decision boundary  
- **Features**: measurable attributes of data  
- **State** (RL): snapshot of environment  
- **Policy**: maps states → actions  

