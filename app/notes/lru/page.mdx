

# LRU Cache: Your Smart, Forgetful Notebook

Imagine you have a small notebook that can only hold a few notes. You use it to jot down things you need to remember right now. Since you have limited space, you need a system to decide what to erase when it gets full.

An LRU (Least Recently Used) Cache works just like this. It's a temporary storage (cache) with a fixed size (capacity). When it's full and you need to add a new item, you discard the item you haven't looked at for the longest timeâ€”the "least recently used" one.

This is incredibly useful in computing. Think of a web browser caching images or a database caching query results. You always want to keep the most relevant data close at hand for speed.

## Table of Contents


## The Intuition: A Two-Part Strategy

To build an efficient LRU Cache, we need to do two things very quickly:

1.  **Find Items Instantly:** When asked for an item, we need to locate it immediately. A **hash map** (or a `dict` in Python) is perfect for this, offering average `O(1)` time lookups.
2.  **Track Recency:** We need to know the exact usage order of all items. A **doubly-linked list** is ideal here because we can move any node to the front or back in `O(1)` time.

Combining these two gives us our blueprint: a hash map for fast lookups and a linked list for fast reordering.

## The Perfect Tool: A Precise Look at `OrderedDict`

While we could build this structure from scratch, Python's `collections` module provides the perfect, ready-made tool: the `OrderedDict`.

It's crucial to be precise here: `OrderedDict` isn't just a dictionary that remembers insertion order. It is a highly optimized class with specific, built-in methods that make it ideal for LRU logic. These methods operate in `O(1)` time, which is the key to our cache's performance.

* `move_to_end(key)`: This built-in function moves an existing key to the right end of the dictionary. We use this to mark an item as **most-recently used** whenever it's accessed.
* `popitem(last=False)`: This built-in function removes and returns the item from the left end (the front) of the dictionary. We use this to evict the **least-recently used** item when the cache is over capacity.

By using these specialized, out-of-the-box methods, we get a clear and highly efficient implementation.

## A Quick Walkthrough

Let's trace the cache's state with a `capacity` of 2. We'll treat the right side of the dictionary as "most recent."

1.  `lru = LRUCache(2)`
    * `cache` is `{}`

2.  `lru.put(1, 10)`
    * `cache` is `{1: 10}`

3.  `lru.put(2, 20)`
    * `cache` is `{1: 10, 2: 20}` (Item 2 is now the most recent)

4.  `lru.get(1)`
    * Returns `10`.
    * The `move_to_end(1)` method is called, making item 1 the most recent.
    * `cache` is now `{2: 20, 1: 10}`

5.  `lru.put(3, 30)`
    * Cache is full. The `popitem(last=False)` method is called.
    * The least recently used item, `2`, is evicted.
    * The new item `3` is added to the end.
    * `cache` is now `{1: 10, 3: 30}`

## Time Complexity
Both `get` and `put` operations run in `O(1)` average time. This excellent performance is a direct result of using the `OrderedDict`'s built-in, constant-time methods (`move_to_end`, `popitem`, and standard dictionary lookups).

## Code Snippet
```python meta="{10,16}/last//move_to_end:4//OrderedDict:1//popitem:5/ lru.py"
from collections import OrderedDict


class LRUCache: 

    def __init__(self, capacity: int):
        self.cache: OrderedDict[int, int] = OrderedDict() 
        self.capacity = capacity

    def get(self, key: int) -> int:
        if (val := self.cache.get(key)) is None:
            return -1
        self.cache.move_to_end(key) 
        return val

    def put(self, key: int, value: int) -> None:
        if key in self.cache:
            self.cache.move_to_end(key)
        self.cache[key] = value
        if len(self.cache) > self.capacity:
            self.cache.popitem(last=False)      
```