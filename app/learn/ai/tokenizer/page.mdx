# Tokenizer: Text to Numbers and Back

## Resizable Table Of Contents

The tokenizer converts text to token IDs (numbers) that the model can process, and converts token IDs back to text. Llama 4 uses a **Tiktoken-based** tokenizer with 202,048 vocabulary tokens.

---

## Overview

```text
Text: "Hello world!"
           │
           ▼
    ┌─────────────┐
    │  TOKENIZER  │  encode()
    └─────────────┘
           │
           ▼
Token IDs: [15496, 1917, 0]
           │
           ▼
      [Transformer]
           │
           ▼
New token ID: 128009
           │
           ▼
    ┌─────────────┐
    │  TOKENIZER  │  decode()
    └─────────────┘
           │
           ▼
Text: "<|eot|>"
```

---

## Byte Pair Encoding (BPE)

Llama 4's tokenizer uses **BPE** — it learns common character sequences from training data and assigns them IDs.

**How BPE works:**
1. Start with individual characters
2. Find the most common pair of adjacent tokens
3. Merge them into a new token
4. Repeat until vocabulary size is reached

**Result:** Common words get single tokens, rare words get split into pieces.

```
"Hello" → [15496]           ← common word, single token
"tokenization" → [...]      ← might be split into "token" + "ization"
"xyz123abc" → [...]         ← rare pattern, multiple tokens
```

---

## Vocabulary Structure

Llama 4's vocabulary has 202,048 tokens ([tokenizer.py:159](../tokenizer.py#L159)):

```python
self.n_words = num_base_tokens + len(special_tokens)  # 202,048
```

| Range | Count | Description |
|-------|-------|-------------|
| 0 - 199,999 | ~200,000 | Base tokens (learned from text) |
| 200,000+ | ~2,048 | Special tokens (see below) |

---

## Special Tokens

Special tokens control model behavior — they're not in the training text but have specific meanings ([tokenizer.py:47-99](../tokenizer.py#L47)):

### Basic Special Tokens (200,000 - 200,004)
```python
BASIC_SPECIAL_TOKENS = [
    "<|begin_of_text|>",  # Start of conversation
    "<|end_of_text|>",    # End of conversation
    "<|fim_prefix|>",     # Fill-in-middle (code completion)
    "<|fim_middle|>",
    "<|fim_suffix|>",
]
```

### Chat/Post-Training Tokens (200,005 - 200,079)
```python
LLAMA4_TEXT_POST_TRAIN_SPECIAL_TOKENS = [
    "<|header_start|>",   # Start of message header
    "<|header_end|>",     # End of message header
    "<|eom|>",            # End of message (expects continuation)
    "<|eot|>",            # End of turn (user's turn now)
    "<|step|>",           # Step marker
    "<|python_start|>",   # Start of Python code block
    "<|python_end|>",     # End of Python code block
    "<|finetune_right_pad|>",  # Padding token
    ...
]
```

### Vision Tokens (200,080 - 201,133)
```python
LLAMA4_VISION_SPECIAL_TOKENS = [
    "<|image_start|>",      # Start of image
    "<|image_end|>",        # End of image
    "<|tile_x_separator|>", # Between horizontal tiles
    "<|tile_y_separator|>", # Between vertical tiles
    "<|image|>",            # Global image marker
    "<|patch|>",            # Individual image patch
    ...
]
```

### Reasoning Tokens (201,134 - 201,143)
```python
LLAMA4_REASONING_SPECIAL_TOKENS = [
    "<|reasoning_thinking_start|>",  # Start of reasoning
    "<|reasoning_thinking_end|>",    # End of reasoning
    ...
]
```

---

## Key Token IDs

Commonly used tokens and their IDs ([tokenizer.py:161-176](../tokenizer.py#L161)):

```python
self.bos_id = self.special_tokens["<|begin_of_text|>"]  # 200,000
self.eos_id = self.special_tokens["<|end_of_text|>"]    # 200,001
self.pad_id = self.special_tokens["<|finetune_right_pad|>"]  # 200,018
self.eot_id = self.special_tokens["<|eot|>"]            # 200,008
self.eom_id = self.special_tokens["<|eom|>"]            # 200,007

self.stop_tokens = [
    self.eos_id,                        # 200,001
    self.special_tokens["<|eom|>"],     # 200,007
    self.special_tokens["<|eot|>"],     # 200,008
]
```

---

## Encoding Text

Convert text to token IDs ([tokenizer.py:178-232](../tokenizer.py#L178)):

```python
def encode(
    self,
    s: str,
    bos: bool,           # prepend <|begin_of_text|>?
    eos: bool,           # append <|end_of_text|>?
    allowed_special: ...,
    disallowed_special: ...,
) -> list[int]:
```

**Example:**
```python
tokenizer.encode("Hello world", bos=True, eos=False)
# → [200000, 15496, 1917]
#    ↑ bos    ↑ Hello  ↑ world
```

**Handling long text:** The tokenizer splits very long strings to avoid memory issues ([tokenizer.py:212-227](../tokenizer.py#L212)):
```python
TIKTOKEN_MAX_ENCODE_CHARS = 400_000
MAX_NO_WHITESPACES_CHARS = 25_000
```

---

## Decoding Tokens

Convert token IDs back to text ([tokenizer.py:234-245](../tokenizer.py#L234)):

```python
def decode(self, t: Sequence[int]) -> str:
    return self.model.decode(cast("list[int]", t))
```

**Example:**
```python
tokenizer.decode([15496, 1917])
# → "Hello world"
```

---

## Chat Formatting

The `ChatFormat` class handles conversation structure ([chat_format.py:64-86](../chat_format.py#L64)):

### Message Structure

Each message is wrapped with headers ([chat_format.py:210-259](../chat_format.py#L210)):

```
<|header_start|>{role}<|header_end|>

{content}<|eot|>
```

**Example encoded message:**
```
RawMessage(role="user", content="Hello")
→ <|header_start|>user<|header_end|>

Hello<|eot|>
```

### Full Conversation

A conversation starts with `<|begin_of_text|>` and ends with an empty assistant header ([chat_format.py:261-300](../chat_format.py#L261)):

```python
def encode_dialog_prompt(self, messages):
    tokens = [self.tokenizer.special_tokens["<|begin_of_text|>"]]
    for message in messages:
        toks, imgs = self.encode_message(message, tool_prompt_format)
        tokens.extend(toks)
    # Prompt model to generate assistant response
    tokens.extend(self._encode_header("assistant"))
    return tokens
```

**Example conversation:**
```text
Messages: [
    {"role": "system", "content": "You are helpful."},
    {"role": "user", "content": "Hi!"},
]

Encoded:
<|begin_of_text|>
<|header_start|>system<|header_end|>

You are helpful.<|eot|>
<|header_start|>user<|header_end|>

Hi!<|eot|>
<|header_start|>assistant<|header_end|>

← model generates here
```

---

## Tool Calls

The model can request tool calls using special formats ([chat_format.py:333-375](../chat_format.py#L333)):

### Python Code Interpreter
```text
<|python_start|>
print("Hello world")
<|python_end|>
```

### Custom Tool (Function Tag Format)
```
<function=weather_lookup>({"city": "Paris"})</function>
```

The `decode_assistant_message_from_content()` function parses these formats and extracts tool calls.

---

## `<|eot|>` vs `<|eom|>`

Both signal the model stopped, but with different implications:

| Token | Meaning | Next Step |
|-------|---------|-----------|
| `\<|eot|>` | End of turn | User should respond |
| `\<|eom|>` | End of message | Model expects to continue (e.g., after tool result) |

**Tool call flow:**
```text
User: "What's the weather?"
Assistant: <function=get_weather>(...)</function><|eom|>  ← expects tool result
Tool: {"temp": 72, "condition": "sunny"}
Assistant: "It's 72°F and sunny!<|eot|>"  ← done, user's turn
```

---

## Tokenizer File

The tokenizer model is stored in `tokenizer.model` (BPE merge rules) and loaded via Tiktoken ([tokenizer.py:129-157](../tokenizer.py#L129)):

```python
def __init__(self, model_path: Path):
    mergeable_ranks = load_bpe_file(model_path)  # load merge rules
    num_base_tokens = len(mergeable_ranks)       # ~200,000

    self.model = tiktoken.Encoding(
        name=model_path.name,
        pat_str=self.O200K_PATTERN,              # regex for splitting
        mergeable_ranks=mergeable_ranks,
        special_tokens=self.special_tokens,
    )
```

---

## Try It

Use the Makefile to experiment with tokenization:

```bash
# See token IDs for text
make tokenize TEXT="Hello world!"
# Output: [15496, 1917, 0]

# See how special tokens are encoded
make tokenize TEXT="<|begin_of_text|>Hello"
```

---

## Summary

```text
┌─────────────────────────────────────────────────────────────────────────────┐
│  TOKENIZER                                                                  │
│                                                                             │
│  vocab_size = 202,048                                                       │
│    └─ ~200,000 base tokens (text patterns)                                  │
│    └─ ~2,048 special tokens (control, vision, reasoning)                    │
│                                                                             │
│  encode(text) → [token IDs]                                                 │
│    └─ BPE: common patterns get single IDs                                   │
│    └─ Optional bos/eos tokens                                               │
│                                                                             │
│  decode([token IDs]) → text                                                 │
│                                                                             │
│  Chat format:                                                               │
│    <|begin_of_text|>                                                        │
│    <|header_start|>{role}<|header_end|>                                     │
│    {content}<|eot|>                                                         │
│    ...                                                                      │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Code Reference

| Component | File | Key Classes/Functions |
|-----------|------|----------------------|
| Tokenizer | [tokenizer.py](../tokenizer.py) | `Tokenizer` |
| Chat format | [chat_format.py](../chat_format.py) | `ChatFormat` |
| Message encoding | [chat_format.py](../chat_format.py) | `encode_message()`, `encode_dialog_prompt()` |
| Tool parsing | [chat_format.py](../chat_format.py) | `decode_assistant_message_from_content()` |
| Special tokens | [tokenizer.py](../tokenizer.py) | `LLAMA4_*_SPECIAL_TOKENS` |
| API types | [api_types.py](../api_types.py) | `RawMessage`, `ToolCall` |
