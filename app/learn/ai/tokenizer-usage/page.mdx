# TikToken-style BPE (LLaMA-4) — Tokenizer Usage

## Resizable Table of Contents

---

## Initialization (`get_instance()`)

Creates a tokenizer instance once.

* Loads `mergeable_ranks`

  * byte-sequence → token-ID table
  * defines all BPE matching and priority (by insertion order)

* Builds `special_tokens`

  * string → token-ID map
  * IDs assigned after all BPE IDs

* Constructs `tiktoken.Encoding` with:

  * `pat_str = O200K_PATTERN`
  * `mergeable_ranks`
  * `special_tokens`

After this point, **all tokenization rules are fixed**.

---

## Encoding (`encode(s)`)

**Purpose:** convert human-readable text into token IDs for the model.

Runs per input string `s`.

Execution order:

1. **Safety splitting (text level)**
   `s` is split into temporary substrings (`substr`) to respect size limits.
   This does not affect tokenization semantics.

2. **Regex chunking (text level)**
   `O200K_PATTERN` is applied inside `tiktoken.Encoding.encode` to each `substr`, producing regex chunks.

3. **UTF-8 encoding (byte level)**
   Each regex chunk is converted to UTF-8 bytes.

4. **Greedy BPE (byte level)**
   Using `mergeable_ranks`:

   * at each byte position, select the longest matching byte sequence
   * if tied, select the earlier-inserted sequence
   * emit its token ID and advance

5. **Optional wrapping**
   `bos_id` is prepended if requested
   `eos_id` is appended if requested

**Result:** a list of token IDs passed to the model.

---

## Decoding (`decode(ids)`)

**Purpose:** convert the model’s token-ID output back into human-readable text.

Runs per token-ID sequence.

Execution order:

* token ID → byte sequence
* concatenate bytes
* UTF-8 decode

No regex, no BPE.

Decoding is lossless.

---

## Invariants

* Tokenizer outputs token IDs (`List[int]`)
* Model receives token IDs
* Encoding is for the model
* Decoding is for humans
* Regex and BPE run only during encoding
* BPE operates on bytes, not text
* Tokenizer behavior is fully defined at initialization

