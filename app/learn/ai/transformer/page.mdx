# The Transformer

## Resizable Table Of Contents

## Running Example

```text
"The cat saw the mouse"  →  predicts "ran", "hid", "squeaked"
"I clicked the mouse"    →  predicts "moved", "opened", "stopped"
```

The token "mouse" (ID 16425) is identical in both inputs. The layers — attention (mixes context across positions) and FFN (transforms each position independently) — refine each token's representation, producing different logits.
    
---

## Overview

The forward pass maps token IDs to logits:

```text
[954, 10338, 13747, 290, 16425]     "The cat saw the mouse"
              ↓
         EMBEDDING              token ID → table lookup → vector
              ↓
      48 TRANSFORMER LAYERS     attention mixes context; FFN transforms per-position
              ↓
       OUTPUT PROJECTION        last position's vector → logits
              ↓
      [202,048 logits]          one score per vocabulary token
```

Sampling from these logits produces: "ran", "hid", "squeaked".

**Logits:** Raw scores, one per vocabulary token. Can be any real number.
**Sampling:** Softmax converts logits to probabilities (sum to 1), then a token is randomly selected weighted by those probabilities.
---

## Stage 1: Embedding

The **embedding table** (`tok_embeddings.weight`) maps each token ID to a learned vector. Shape: [vocab_size × dim] = [202,048 × 5,120].

```text
Input:  [954, 10338, 13747, 290, 16425]

                    ↓ table lookup

Output: 5 vectors, each 5,120 dimensions

        Position 0: [0.01, -0.02, 0.03, ...]    ← row 954 ("The")
        Position 1: [-0.002, -0.013, 0.005, ...] ← row 10338 ("cat")
        Position 2: [...]                        ← row 13747 ("saw")
        Position 3: [...]                        ← row 290 ("the")
        Position 4: [-0.002, 0.020, -0.004, ...] ← row 16425 ("mouse")
```

Token 16425 ("mouse") maps to the same vector regardless of context. The layers add context.

---

## Stage 2: Layers

Each of the 48 layers has two operations:

1. **Attention** — each position aggregates information from other positions
2. **FFN/MoE** — each position is transformed independently

### Attention

Attention allows each position to incorporate information from other positions. Each position computes a weighted sum of all (causally visible) positions' values.

#### Step 1: Create Q, K, V

Each position's vector is projected into three vectors:

- **Q** (query): used to compute attention scores against other positions
- **K** (key): used to compute attention scores from other positions
- **V** (value): the content that gets aggregated

```text
vector × Wq  →  Q    [5,120 → 5,120]
vector × Wk  →  K    [5,120 → 1,024]  (smaller due to GQA)
vector × Wv  →  V    [5,120 → 1,024]  (smaller due to GQA)
```

Wq, Wk, Wv are learned weight matrices. All positions use the same matrices.

#### Step 2: Add Position Information (RoPE)

Q and K are computed identically at every position — without position encoding, the attention scores between "cat chased mouse" and "mouse chased cat" would be identical.

RoPE (Rotary Position Embedding) encodes position by rotating Q and K vectors based on their position index. Position 0 is rotated by angle 0, position 1 by angle θ, position 2 by 2θ, etc.

When computing Q₄ · K₁, the rotation angles (4θ and 1θ) produce a dot product that depends on their difference (3θ). This encodes **relative position** — "3 positions apart" — regardless of absolute position. (For the rotation math, see [rope-deep-dive.md](rope-deep-dive.md).)

#### Step 3: Compute Attention Scores

Attention scores are dot products between Q and K vectors:

```text
Position 4 ("mouse") computes:

Q₄ · K₀ ("The")   = 0.1
Q₄ · K₁ ("cat")   = 0.8   ← highest
Q₄ · K₂ ("saw")   = 0.6
Q₄ · K₃ ("the")   = 0.1
Q₄ · K₄ ("mouse") = 0.5
```

Higher dot product → larger weight in the final sum.

#### Step 4: Mask and Softmax

**Masking** sets attention scores to -∞ before softmax, forcing those weights to zero.

**Causal mask** (all layers): Position i cannot attend to positions > i.

**Chunked mask** (RoPE layers only): Position i can only attend within its chunk.

Llama 4 uses **iRoPE** — most layers apply RoPE and use chunked masking; every Nth layer skips RoPE (NoPE) and uses only the causal mask, allowing global attention across the full sequence.

**Softmax** converts scores to weights that sum to 1:

```text
Position 4's attention weights (after softmax):
  pos 0: 0.10    pos 1: 0.45    pos 2: 0.20    pos 3: 0.05    pos 4: 0.20
         "The"         "cat"          "saw"          "the"         "mouse"
```

#### Step 5: Weighted Sum of Values

The output for each position is a weighted sum of V vectors:

```text
Output₄ = 0.10×V₀ + 0.45×V₁ + 0.20×V₂ + 0.05×V₃ + 0.20×V₄
```

Position 4's output vector is 45% V₁ ("cat"), 20% V₂ ("saw"), etc. All positions compute this in parallel.

#### Combining Heads

In practice, attention runs in parallel across 40 "heads," each working on 128 dimensions. After attention, we concatenate all heads and multiply by another weight matrix (Wo) to mix information across heads.

For details on multi-head attention, grouped query attention (GQA), and KV caching, see [attention-deep-dive.md](attention-deep-dive.md).

### FFN / MoE

**FFN** (Feed-Forward Network) transforms each position independently using SwiGLU:

```text
output = (SiLU(x @ W1) ⊙ (x @ W3)) @ W2
```

Two parallel projections (W1, W3) expand to a higher dimension. One passes through SiLU activation, then element-wise multiply (⊙), then W2 projects back. This applies element-wise nonlinearity to each position's vector.

**MoE** (Mixture of Experts) uses multiple FFNs ("experts") instead of one:
- A router (linear layer) scores each expert for the input
- The top-scoring expert is selected (top_k=1)
- A shared expert also processes every token
- Output = shared expert output + (router score × selected expert output)

MoE increases capacity without proportional compute — each token activates the shared expert plus 1 routed expert (out of 16 in Scout, 128 in Maverick).

For details, see [ffn-moe-deep-dive.md](ffn-moe-deep-dive.md).

### One Complete Layer

For each position's vector x (5,120 dims):
1. RMSNorm(x) → attention → **add** result to x → call this h
2. RMSNorm(h) → FFN/MoE → **add** result to h → output

RMSNorm scales the vector so its root-mean-square equals 1. The additions are residual connections — each operation's output is added to its input, preserving the original signal.

Each layer's output becomes the next layer's input. After 48 layers, position 4's vector encodes "mouse" in the context of the preceding tokens.

---

## Stage 3: Output

Due to causal masking, each position only attends to earlier positions. The last position (position 4) is the only one whose vector has aggregated information from the entire sequence — so we project that vector to vocabulary size:

```text
final_vector [5,120]  ×  output.weight [202,048 × 5,120]  →  [202,048 logits]
```

Each logit corresponds to one vocabulary token. Higher logit → higher probability after softmax. Sampling from these probabilities produces the next token.

---

## Summary

```text
"The cat saw the mouse"
         ↓
    [5 token IDs]
         ↓
┌─────────────────────────────────────────────────────────────┐
│  EMBEDDING                                                  │
│  Token ID → look up row in embedding table → vector         │
│  Output: 5 vectors, each 5,120 dimensions                   │
└─────────────────────────────────────────────────────────────┘
         ↓
┌─────────────────────────────────────────────────────────────┐
│  LAYER 0                                                    │
│                                                             │
│  Attention: mix information across positions                │
│      → Q·K scores → softmax → weighted sum of V             │
│      → add to input (residual)                              │
│                                                             │
│  FFN/MoE: transform each position independently             │
│      → add to input (residual)                              │
│                                                             │
└─────────────────────────────────────────────────────────────┘
         ↓
      ... layers 1-46 ...
         ↓
┌─────────────────────────────────────────────────────────────┐
│  LAYER 47                                                   │
│  (same structure as layer 0, different learned weights)     │
└─────────────────────────────────────────────────────────────┘
         ↓
┌─────────────────────────────────────────────────────────────┐
│  OUTPUT PROJECTION                                          │
│  Last position's vector × output.weight → 202,048 logits    │
└─────────────────────────────────────────────────────────────┘
         ↓
    Sample from logits → "ran", "hid", "squeaked"
```

The key insight: **the layers transform context-free embeddings into context-aware representations.** Before the layers, "mouse" is always the same vector. After 48 layers, "mouse" in "The cat saw the mouse" is a different vector from "mouse" in "I clicked the mouse."

---

## Reference

### Key Parameters

| Parameter    | Scout   | Maverick | Meaning                          |
| ------------ | ------- | -------- | -------------------------------- |
| dim          | 5,120   | 5,120    | Vector size                      |
| n_layers     | 48      | 48       | Number of layers                 |
| n_heads      | 40      | 40       | Attention heads (Q)              |
| n_kv_heads   | 8       | 8        | K/V heads (GQA reduces memory)   |
| num_experts  | 16      | 128      | Experts per MoE layer            |
| vocab_size   | 202,048 | 202,048  | Vocabulary size                  |
| total_params | 109B    | 400B     | Total parameters                 |
| active_params| 17B     | 17B      | Parameters used per token        |

### Code Locations

| Component   | File                              | Function                         |
| ----------- | --------------------------------- | -------------------------------- |
| Embedding   | [model.py](../model.py)           | `Transformer.tok_embeddings`     |
| Attention   | [model.py](../model.py)           | `Attention.forward()`            |
| RoPE        | [model.py](../model.py)           | `apply_rotary_emb()`             |
| FFN         | [ffn.py](../ffn.py)               | `FeedForward.forward()`          |
| MoE         | [moe.py](../moe.py)               | `MoE.forward()`                  |
| Generation  | [generation.py](../generation.py) | `Llama4.generate()`              |

### Checkpoint Structure

The model's learned weights are stored in `.pth` files:

```text
tok_embeddings.weight         [202,048 × 5,120]   embedding table
layers.0.attention.wq.weight  [5,120 × 5,120]     Q projection
layers.0.attention.wk.weight  [1,024 × 5,120]     K projection (smaller due to GQA)
layers.0.attention.wv.weight  [1,024 × 5,120]     V projection (smaller due to GQA)
layers.0.attention.wo.weight  [5,120 × 5,120]     output projection
layers.0.feed_forward.mlp.fc1_weight  [27,306 × 5,120]   w1+w3 stacked (split on load)
layers.0.feed_forward.mlp.fc2_weight  [5,120 × 13,653]   w2 (project back)
...
layers.47.*                                       (48 layers total)
output.weight                 [202,048 × 5,120]   final projection
```

Explore with: `make inspect`
