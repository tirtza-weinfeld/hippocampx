# AI & Language Models

## Resizable Table Of Contents

## Architecture Overview

```text
┌─────────────────────────────────────────────────────────────────────────────┐
│                              LLAMA 4 INFERENCE                              │
└─────────────────────────────────────────────────────────────────────────────┘

  Input: "What is 2+2?"              Images (optional)
              │                            │
              ▼                            ▼
       ┌─────────────┐              ┌─────────────┐
       │  TOKENIZER  │              │   VISION    │
       │             │              │   ENCODER   │
       └─────────────┘              └─────────────┘
              │                            │
              │    ┌───────────────────────┘
              ▼    ▼
       ┌─────────────────────────────────────┐
       │            TRANSFORMER              │
       │  ┌───────────────────────────────┐  │
       │  │  Attention (iRoPE + GQA)      │  │
       │  │  + KV Cache                   │  │
       │  ├───────────────────────────────┤  │
       │  │  MoE / FFN                    │  │
       │  │  (Mixture of Experts)         │  │
       │  └───────────────────────────────┘  │
       │         × 48 layers                 │
       └─────────────────────────────────────┘
              │
              ▼
       ┌─────────────┐
       │ GENERATION  │  Temperature + Top-p sampling
       └─────────────┘
              │
              ▼
  Output: "4"
```

---

## Documentation Map

| Document | What It Covers | Read When You Want To... |
|----------|----------------|--------------------------|
| [Generation](ai/generation) , [](https://github.com/tirtza-weinfeld/hippocampx/tree/main/app/learn/ai/generation/page.mdx)|Autoregressive loop, temperature, top-p sampling | Understand how text is generated token-by-token |
| [Quantization](ai/quantization) ,[](https://github.com/tirtza-weinfeld/hippocampx/tree/main/app/learn/ai/quantization/page.mdx)|INT4/FP8 compression, memory optimization | Deploy on limited GPU memory |
| [Tokenizer](ai/tokenizer) , [](https://github.com/tirtza-weinfeld/hippocampx/tree/main/app/learn/ai/tokenizer/page.mdx)| BPE encoding, special tokens, chat formatting | Understand text ↔ token ID conversion |
| [Transformer](ai/transformer) , [](https://github.com/tirtza-weinfeld/hippocampx/tree/main/app/learn/ai/transformer/page.mdx) |Core architecture: attention, RoPE, GQA, MoE, KV cache | Understand how the model processes tokens |
| [Vision](ai/vision) , [](https://github.com/tirtza-weinfeld/hippocampx/tree/main/app/learn/ai/vision/page.mdx)|ViT encoder, patch embedding, pixel shuffle | Understand image processing pipeline |
| [PyTorch](ai/pytorch) , [](https://github.com/tirtza-weinfeld/hippocampx/tree/main/app/learn/ai/pytorch/page.mdx)|||

---

## Quick Start

### Running Inference

```bash
# Install dependencies
pip install -r requirements.txt

# Chat completion (single H100 with INT4 quantization)
torchrun --nproc_per_node=1 -m broca.scripts.chat_completion \
    --checkpoint_dir /path/to/Llama-4-Scout-17B-16E-Instruct \
    --quantization_mode int4_mixed \
    --max_seq_len 4096
```

### Local Debugging (CPU, no GPU required)

```bash
# Create tiny checkpoint with random weights
python -m broca.scripts.create_debug_checkpoint --output_dir debug_checkpoint

# Run on CPU for learning/debugging
python -m broca.scripts.debug_local --checkpoint_dir debug_checkpoint
```

See [CLAUDE.md](../CLAUDE.md) for VS Code debugging setup.

---

## Model Variants

| Model | Parameters | Experts | VRAM (BF16) | VRAM (INT4) |
|-------|------------|---------|-------------|-------------|
| Scout | 17B active / 109B total | 16 | 218 GB | ~55 GB |
| Maverick | 17B active / 400B+ total | 128 | 800+ GB | ~200 GB |

Both models share the same architecture — they differ only in expert count.

---

## Code Structure

```text
broca/
├── generation.py      # Entry point, autoregressive loop
├── model.py           # Transformer, Attention, TransformerBlock
├── moe.py             # Mixture of Experts routing
├── ffn.py             # SwiGLU feed-forward network
├── tokenizer.py       # Tiktoken-based tokenizer
├── chat_format.py     # Message encoding, tool parsing
├── args.py            # Model configuration (ModelArgs, MoEArgs)
├── api_types.py       # Data types (RawMessage, GenerationResult)
├── vision/
│   ├── encoder.py     # Vision Transformer (ViT)
│   └── embedding.py   # Pixel shuffle, vision adapter
├── quantization/
│   └── loader.py      # INT4/FP8 weight conversion
└── scripts/
    ├── chat_completion.py
    ├── completion.py
    └── debug_local.py
```

---

## Suggested Reading Order

1. **[transformer.md](transformer.md)** — Start here. Covers the core architecture.
2. **[generation.md](generation.md)** — How tokens are sampled and generated.
3. **[tokenizer.md](tokenizer.md)** — How text becomes tokens.
4. **[vision.md](vision.md)** — Optional, if working with images.
5. **[quantization.md](quantization.md)** — Optional, if deploying to limited hardware.

---

## Key Concepts Quick Reference

| Concept | What It Is |
|---------|------------|
| [iRoPE](ai/transformer#irope) | Interleaved RoPE — alternates RoPE and NoPE layers |
| [GQA](ai/transformer#grouped-query-attention-gqa)| Grouped Query Attention — shares K/V across heads | 
| [MoE](ai/transformer#mixture-of-experts-moe) | Mixture of Experts — sparse activation | 
| [KV Cachet](ai/transformer#kv-cache) | Stores past K/V to avoid recomputation |
| [Top-p](ai/generation#top-p-nucleus-sampling) | Nucleus sampling — filters low-prob tokens |
| [BPE](ai/tokenizer#byte-pair-encoding-bpe) | Byte Pair Encoding — tokenization algorithm | 

