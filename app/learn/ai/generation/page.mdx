# Text Generation: How Tokens Are Produced

## Resizable Table Of Contents

This document explains how Llama 4 generates text one token at a time, including temperature sampling, top-p filtering, and the autoregressive loop.

---

## Overview

Text generation is **autoregressive**: generate one token, append it to the input, run the model again to get the next token, repeat. Each new token depends on all previous tokens.

```text
Input:   "The cat sat on the"
Step 1:  → model predicts "mat"  → append → "The cat sat on the mat"
Step 2:  → model predicts "."    → append → "The cat sat on the mat."
Step 3:  → model predicts "<|eot|>" → STOP (end-of-turn token)
```

The model doesn't "think ahead" — it only sees what comes before the current position.

---

## The Autoregressive Loop

The main generation loop ([generation.py:321-404](../generation.py#L321)):

```python
prev_pos = 0
for cur_pos in range(min_prompt_len, total_len):
    # 1. Run transformer forward pass
    xformer_output = self.model.forward(xformer_input)
    logits = xformer_output.logits  # [batch, seq_len, vocab_size]

    # 2. Sample next token from logits
    if temperature > 0:
        probs = torch.softmax(logits[:, -1] / temperature, dim=-1)
        next_token = sample_top_p(probs, top_p)
    else:
        next_token = torch.argmax(logits[:, -1], dim=-1)  # greedy

    # 3. Place token in buffer
    tokens[:, cur_pos] = next_token

    # 4. Check for stop tokens
    eos_reached |= torch.isin(next_token, stop_tokens)

    # 5. Update position for KV cache
    prev_pos = cur_pos

    if all(eos_reached):
        break
```

**Key variables:**
- `prev_pos`: Where we last ran the model (used by KV cache)
- `cur_pos`: Current generation position
- `tokens`: Buffer holding prompt + generated tokens
- `eos_reached`: Tracks which conversations have finished

---

## Logits: Raw Model Output

The transformer outputs **logits** — one score per vocabulary token (202,048 scores for Llama 4).

```
logits = [2.1, -0.5, 0.8, 1.3, ...]  ← 202,048 raw scores
          ↑ token 0  ↑ token 1  ...

Higher score = model thinks this token is more likely to come next
```

Logits are not probabilities (they can be negative, don't sum to 1). We convert them to probabilities using softmax.

---

## Temperature: Controlling Randomness

Temperature controls how "spread out" the probability distribution is ([generation.py:355](../generation.py#L355)):

```python
probs = torch.softmax(logits[:, -1] / temperature, dim=-1)
```

**How it works:**

```text
Original logits:  [2.0, 1.0, 0.5, 0.1]

temperature=1.0:
  softmax([2.0, 1.0, 0.5, 0.1]) → [0.45, 0.17, 0.10, 0.07]  (normal spread)

temperature=0.5 (divide by 0.5 = multiply by 2):
  softmax([4.0, 2.0, 1.0, 0.2]) → [0.78, 0.11, 0.04, 0.02]  (sharper, top token dominates)

temperature=2.0 (divide by 2):
  softmax([1.0, 0.5, 0.25, 0.05]) → [0.35, 0.21, 0.16, 0.13]  (flatter, more random)
```

**Guidelines:**
- `temperature=0`: Greedy — always pick highest score (deterministic)
- `temperature=0.6`: Default — balanced creativity/coherence
- `temperature=1.0`: Full randomness according to model's learned distribution
- `temperature>1.0`: More random — can produce incoherent output

---

## Top-p (Nucleus) Sampling

Top-p filters out low-probability tokens before sampling ([generation.py:473-495](../generation.py#L473)):

```python
def sample_top_p(probs: torch.Tensor, p: float) -> torch.Tensor:
    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)
    probs_sum = torch.cumsum(probs_sort, dim=-1)
    mask = probs_sum - probs_sort > p
    probs_sort[mask] = 0.0
    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))  # renormalize
    next_token = torch.multinomial(probs_sort, num_samples=1)
    next_token = torch.gather(probs_idx, -1, next_token)
    return next_token
```

**Step by step:**

```text
Sorted probs: [0.40, 0.25, 0.15, 0.10, 0.05, 0.03, 0.02]
Cumsum:       [0.40, 0.65, 0.80, 0.90, 0.95, 0.98, 1.00]

With top_p=0.9:
  - Token 1: cumsum=0.40, include (0.40 <= 0.9)
  - Token 2: cumsum=0.65, include (0.65 <= 0.9)
  - Token 3: cumsum=0.80, include (0.80 <= 0.9)
  - Token 4: cumsum=0.90, include (0.90 <= 0.9)
  - Token 5+: exclude (cumsum > 0.9)

Result: Only sample from top 4 tokens, renormalized to sum to 1.0
```

**Why top-p?** Prevents sampling rare, nonsensical tokens while allowing dynamic vocabulary based on the model's confidence. When the model is confident (one token dominates), top-p allows few options. When uncertain, more options are available.

---

## Greedy vs Sampling

**Greedy** (`temperature=0`) ([generation.py:360](../generation.py#L360)):
```python
next_token = torch.argmax(logits[:, -1], dim=-1)
```
Always picks the highest-scoring token. Deterministic — same input always produces same output.

**Sampling** (`temperature>0`):
```python
probs = torch.softmax(logits[:, -1] / temperature, dim=-1)
next_token = sample_top_p(probs, top_p)
```
Randomly samples according to probability distribution. Same input can produce different outputs.

**When to use which:**
- Greedy: Factual questions, code generation, structured output
- Sampling: Creative writing, brainstorming, conversational variety

---

## Stop Tokens

Generation stops when the model outputs certain special tokens ([generation.py:316](../generation.py#L316), [tokenizer.py:172-176](../tokenizer.py#L172)):

```python
stop_tokens = [
    self.eos_id,                        # <|end_of_text|> (id: 200001)
    self.special_tokens["<|eom|>"],     # end of message (id: 200007)
    self.special_tokens["<|eot|>"],     # end of turn (id: 200008)
]
```

**Difference:**
- `<|eot|>` (end of turn): Model finished responding, user's turn to speak
- `<|eom|>` (end of message): Model finished current message but expects to continue (e.g., after tool call)
- `<|end_of_text|>`: Absolute end of conversation

---

## Batch Processing

Multiple conversations can run in parallel on the same GPU ([generation.py:266-267](../generation.py#L266)):

```python
bsz = len(llm_inputs)  # batch size
assert bsz <= params.max_batch_size

# Token buffer: [batch_size, total_len]
tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=device)
```

**Example with 3 conversations:**

```
Conversation 0: "Hello"        → generates "Hi there!"
Conversation 1: "What is 2+2?" → generates "4"
Conversation 2: "Tell me a joke" → generates "Why did..."

All run simultaneously, each at its own pace.
```

Different conversations may have different prompt lengths. The `input_text_mask` tracks where real tokens are vs padding:

```python
input_text_mask = tokens != pad_id  # True where real tokens exist
next_token = torch.where(input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token)
```

This ensures we don't overwrite prompt tokens — only fill in padding positions.

---

## KV Cache Efficiency

The loop uses `prev_pos` and `start_pos` to leverage the KV cache ([generation.py:341-342](../generation.py#L341)):

```python
xformer_input = TransformerInput(
    tokens=tokens[:, prev_pos:cur_pos],  # only NEW tokens
    tokens_position=prev_pos,            # where in sequence
)
```

**First iteration (prompt):**
- `prev_pos=0`, `cur_pos=5` (if prompt has 5 tokens)
- Process all 5 tokens, cache their K/V

**Subsequent iterations:**
- `prev_pos=5`, `cur_pos=6`
- Process only 1 token (the new one), use cached K/V for positions 0-4

This is what makes generation O(n) instead of O(n²) per token.

---

## Image Handling (Multimodal)

On the first iteration, images are processed and embedded ([generation.py:326-335](../generation.py#L326)):

```python
if prev_pos == 0 and any(inp.images is not None for inp in llm_inputs):
    image_mask = tokens[:, prev_pos:cur_pos] == self.tokenizer.special_tokens["<|patch|>"]
    h = self.model.tok_embeddings(tokens[:, prev_pos:cur_pos])
    image_embedding = MaskedEmbedding(
        embedding=self.model.vision_embeddings(image_batch, image_mask, h),
        mask=image_mask,
    )
```

The vision encoder produces embeddings that replace `<|patch|>` placeholder tokens in the sequence.

---

## API: chat_completion vs completion

**chat_completion** ([generation.py:428-470](../generation.py#L428)): For multi-turn conversations with system/user/assistant roles.

```python
messages = [
    {"role": "system", "content": "You are helpful."},
    {"role": "user", "content": "What's 2+2?"},
]
for result in llama.chat_completion([messages]):
    print(result[0].text, end="")
```

**completion** ([generation.py:406-426](../generation.py#L406)): For raw text completion without chat formatting.

```python
for result in llama.completion(["Once upon a time"]):
    print(result[0].text, end="")
```

Both are thin wrappers around `generate()`.

---

## Streaming Results

`generate()` is a Python generator that yields results token by token ([generation.py:381-398](../generation.py#L381)):

```python
yield results  # Stream results back to caller
```

Each yield contains a list of `GenerationResult` objects (one per conversation in the batch):

```python
@dataclass
class GenerationResult:
    token: int          # Token ID (e.g., 13347)
    text: str           # Decoded text (e.g., "Hello")
    source: str         # "input" or "output"
    batch_idx: int      # Which conversation this belongs to
    finished: bool      # True if this conversation hit a stop token
    logprobs: list[float] | None  # Log probabilities (if requested)
```

---

## Summary

```text
┌─────────────────────────────────────────────────────────────────────────────┐
│  GENERATION LOOP                                                            │
│                                                                             │
│  tokens = [prompt tokens] + [padding for generation]                        │
│                                                                             │
│  for each position after prompt:                                            │
│    1. Forward pass → logits [vocab_size scores]                             │
│    2. Temperature scaling → softmax → probabilities                         │
│    3. Top-p filtering → remove unlikely tokens                              │
│    4. Sample or argmax → next_token                                         │
│    5. Place in buffer, check stop tokens                                    │
│    6. Yield result (streaming)                                              │
│    7. Update prev_pos for KV cache                                          │
│                                                                             │
│  Stop when all conversations hit stop tokens or max_gen_len                 │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Code Reference

| Component | File | Key Functions |
|-----------|------|---------------|
| Main loop | [generation.py](../generation.py) | `Llama4.generate()` |
| Top-p sampling | [generation.py](../generation.py) | `sample_top_p()` |
| Chat API | [generation.py](../generation.py) | `Llama4.chat_completion()` |
| Stop tokens | [tokenizer.py](../tokenizer.py) | `Tokenizer.stop_tokens` |
| Result type | [api_types.py](../api_types.py) | `GenerationResult` |
