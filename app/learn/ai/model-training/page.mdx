# Model Training

## Resizable Table of Contents

Where the model learns to predict language.

## What Gets Learned

All weights are learned **jointly** via gradient descent:

- **Embeddings** — Dense vectors representing each token
- **Attention weights** — Which tokens attend to which
- **MLP weights** — Feed-forward transformations in each layer

## The Training Loop

```
for each batch of text:
    1. Tokenize → token IDs
    2. Forward pass → predict next token
    3. Compare to actual → compute loss
    4. Backward pass → compute gradients
    5. Update weights → gradient descent
```

## Core Concepts

### Embeddings

Each token ID maps to a learned vector. Similar tokens end up with similar vectors.

### Attention

"Which other tokens should I look at to predict the next one?"

Self-attention lets each position attend to all previous positions (within the context window).

### Gradient Descent

The optimizer adjusts weights to reduce prediction error:
- Loss too high → adjust weights
- Repeat billions of times

### Context Window

Fixed architectural limit — how many tokens the model can "see" at once.

- GPT-4: 8K–128K tokens
- Claude: 200K tokens

## Key Points

- Training takes weeks/months on thousands of GPUs
- Weights are frozen **after** training
- Context window is fixed, not learned
